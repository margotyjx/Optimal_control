{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25f3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff05045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original potential function\n",
    "class functions:\n",
    "    def __init__(self,beta,alpha):\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def funU(self,x):\n",
    "        U = 0.5*self.beta*(x**2) + 1/4*self.alpha*(x**4)\n",
    "\n",
    "        return U\n",
    "\n",
    "    def funT(self,p):\n",
    "        T = 0.5*(p**2)\n",
    "        return T\n",
    "\n",
    "    def dU(self,x):\n",
    "        dUx = x*(self.beta + self.alpha * (x**2))\n",
    "\n",
    "        return dUx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c65f4d5",
   "metadata": {},
   "source": [
    "## NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6db7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        # hidden layer\n",
    "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        # Get intermediate outputs using hidden layer\n",
    "        out = self.linear1(xb)\n",
    "        # Apply activation function\n",
    "        tanhf = nn.Tanh()\n",
    "        out = tanhf(out)\n",
    "        # Get predictions using output layer\n",
    "        out = self.linear2(out)\n",
    "        # apply activation function again\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b5c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all training data from files\n",
    "# train_data = torch.load('train_data_beta20.pt')\n",
    "# train_data.requires_grad_(True)\n",
    "# plt.scatter(train_data[:,0].detach().numpy(), train_data[:,1].detach().numpy(),s = 0.1)\n",
    "# plt.xlim([-4,4])\n",
    "# plt.ylim([-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac10aaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160000, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-4.0, 4.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uniform trainning points\n",
    "x = np.linspace(-2.5,2.5,400)\n",
    "y = np.linspace(-2,2,400)\n",
    "x_train, y_train = np.meshgrid(x,y)\n",
    "train_data = torch.tensor(np.hstack((x_train.ravel()[:,None],y_train.ravel()[:,None])),\\\n",
    "                         dtype = torch.float32)\n",
    "train_data.requires_grad_(True)\n",
    "print(train_data.shape)\n",
    "plt.scatter(train_data[:,0].detach().numpy(), train_data[:,1].detach().numpy(),s = 0.1)\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a9747ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary points of A will be denoted as 1, boundary points of B will be 2\n",
    "# non boundary points will be 0\n",
    "category = np.zeros(train_data.shape[0])\n",
    "\n",
    "for i in range(train_data.shape[0]):\n",
    "    a = torch.tensor([-1,0])\n",
    "    b = torch.tensor([1,0])\n",
    "    rx = 0.3\n",
    "    ry = 0.4\n",
    "    curr_xy = train_data[i,:]\n",
    "    distB = (curr_xy - b).pow(2)/torch.tensor([0.3**2, 0.4**2])\n",
    "    distA = (curr_xy - a).pow(2)/torch.tensor([0.3**2, 0.4**2])\n",
    "    \n",
    "    if distA.sum() <= 1:\n",
    "        category[i] = 1\n",
    "    elif distB.sum() <= 1:\n",
    "        category[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f8950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aset = np.argwhere(category == 1)\n",
    "Bset = np.argwhere(category == 2)\n",
    "inset = np.argwhere(category == 0)\n",
    "train_Aset = train_data[Aset[:,0],:]\n",
    "train_Bset = train_data[Bset[:,0],:]\n",
    "train_inset = train_data[inset[:,0],:]\n",
    "torch.save(train_Aset,'trainA.pt')\n",
    "torch.save(train_Bset,'trainB.pt')\n",
    "torch.save(train_inset,'train_in.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a7f96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5000, -2.0000],\n",
       "        [-2.4875, -2.0000],\n",
       "        [-2.4749, -2.0000],\n",
       "        ...,\n",
       "        [ 2.4749,  2.0000],\n",
       "        [ 2.4875,  2.0000],\n",
       "        [ 2.5000,  2.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Aset = torch.load('trainA.pt')\n",
    "train_Bset = torch.load('trainB.pt')\n",
    "train_inset = torch.load('train_in.pt')\n",
    "train_Aset.requires_grad = True\n",
    "train_Bset.requires_grad = True\n",
    "train_inset.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6658ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "N_neurons = 40\n",
    "# models for 4 different neuron numbers\n",
    "torch.manual_seed(1)\n",
    "model = Model1(input_size,N_neurons,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ada89da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "size1,size2 = train_inset.shape\n",
    "rhs = torch.zeros(size1,)\n",
    "rhsA = torch.zeros(train_Aset.shape[0],1)\n",
    "rhsB = torch.ones(train_Bset.shape[0],1)\n",
    "train_ds = TensorDataset(train_inset,rhs)\n",
    "batch_size = int(size1/50) # the batch size is the size of the training data\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "optimizer1 = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6b163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.tensor(-1)\n",
    "alpha = torch.tensor(1)\n",
    "gamma = torch.tensor(0.5)\n",
    "Funs = functions(beta,alpha)\n",
    "betaT = torch.tensor(40)\n",
    "# betaT = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40de3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(500):\n",
    "    for X,y in train_dl:\n",
    "        #X[:,0] are positions, X[:,1] are momentum\n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "    \n",
    "        Q = model(X)\n",
    "        QA = model(train_Aset)\n",
    "        QB = model(train_Bset)\n",
    "        U = Funs.funU(X[:,0]) # potential energy function\n",
    "        T = Funs.funT(X[:,1]) # kinetic energy function\n",
    "        gradU = Funs.dU(X[:,0])\n",
    "        \n",
    "    \n",
    "        derivQ = torch.autograd.grad(Q,X,allow_unused=True, retain_graph=True, grad_outputs = torch.ones_like(Q), create_graph=True)\n",
    "        dQ = derivQ[0]\n",
    "        \n",
    "        deriv_yx_yy = torch.autograd.grad(dQ[:,1], X,allow_unused=True,grad_outputs=torch.ones_like(dQ[:,0]), \\\n",
    "        retain_graph=True, create_graph=True)\n",
    "        dqqQ = deriv_yx_yy[0][:,1]\n",
    "        \n",
    "        Lq = X[:,1]*dQ[:,0] - gradU*dQ[:,1] - gamma * X[:,1]*dQ[:,1] + gamma/betaT*dqqQ\n",
    "        \n",
    "        lossin = loss_fn(Lq,y)\n",
    "        lossA = loss_fn(QA,rhsA)\n",
    "        lossB = loss_fn(QB,rhsB)\n",
    "        \n",
    "        loss = lossin + 0.5*lossA + 0.5*lossB\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "    if epoch%25 == 0:\n",
    "        print('Epoch: {}, Loss interior pts: {:.4f}, Loss on A: {:.4f}, Loss on B: {:.4f}'.format(\n",
    "        epoch, lossin, lossA, lossB))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf2634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,'Duffiing_gamma0.5_beta20_PINN_uniform.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = model(train_data)\n",
    "plt.scatter(train_data[:,0].detach().numpy(), train_data[:,1].detach().numpy(),\n",
    "           c = Q.detach().numpy(), s = 0.1)\n",
    "plt.colorbar()\n",
    "# plt.savefig('Duffing_oscillator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece8fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_half = np.array([])\n",
    "P_half = np.array([])\n",
    "for i in range(train_data.shape[0]):\n",
    "    if torch.abs(Q[i] - 0.5) < 0.005:\n",
    "        X_half = np.append(X_half, train_data[i,0].detach().numpy())\n",
    "        P_half = np.append(P_half, train_data[i,1].detach().numpy())\n",
    "\n",
    "        \n",
    "plt.scatter(train_data[:,0].detach().numpy(), train_data[:,1].detach().numpy(),\n",
    "           c = Q.detach().numpy(), s = 0.1)\n",
    "plt.scatter(X_half,P_half,s = 0.5,c = 'r')\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "plt.colorbar()\n",
    "# plt.savefig('isocommittor_DuffingOscillator_gamma0.5-2hidden_PINN.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a1503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
